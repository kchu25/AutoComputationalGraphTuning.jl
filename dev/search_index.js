var documenterSearchIndex = {"docs":
[{"location":"#AutoComputationalGraphTuning","page":"Home","title":"AutoComputationalGraphTuning","text":"Documentation for AutoComputationalGraphTuning.","category":"section"},{"location":"#Quick-Start","page":"Home","title":"Quick Start","text":"This package makes hyperparameter tuning for Flux models simple. You just need a data struct and a model creation function.","category":"section"},{"location":"#What-You-Need","page":"Home","title":"What You Need","text":"Data struct with four fields:\n\ndata.X, data.Y - your features and labels\ndata.X_dim, data.Y_dim - dimensions of each feature/label\n\nModel function that creates a Flux model:\n\nfunction create_model(X_dim, Y_dim, batch_size; rng, use_cuda)\n    # Your model code here\n    return model  # Must have a `linear_sum` property\nend","category":"section"},{"location":"#Loss-Function-Flexibility","page":"Home","title":"Loss Function Flexibility","text":"Want to experiment with different loss functions? Easy:\n\n# Default MSE\ntune_hyperparameters(data, create_model)\n\n# Try MAE instead (less sensitive to outliers)\ntune_hyperparameters(data, create_model; \n                    loss_fcn=(loss=Flux.mae, agg=StatsBase.mean))\n\n# Huber loss (robust choice)\ntune_hyperparameters(data, create_model;\n                    loss_fcn=(loss=Flux.huber_loss, agg=StatsBase.mean))\n\nThe loss_fcn parameter takes any Flux loss function with your choice of aggregation. Works the same way in train_final_model() too.","category":"section"},{"location":"#Basic-Workflow","page":"Home","title":"Basic Workflow","text":"# 1. Tune hyperparameters\nresults = tune_hyperparameters(data, create_model; \n                              max_epochs=50, n_trials=100)\n\n# 2. Train final model  \nmodel, stats = train_final_model(data, create_model; \n                                seed=42, max_epochs=100)\n\nThe package handles data splitting, normalization, early stopping, and model selection automatically.\n\n","category":"section"},{"location":"#AutoComputationalGraphTuning.DataSplit","page":"Home","title":"AutoComputationalGraphTuning.DataSplit","text":"DataSplit\n\nRepresents a single data split with tensor and labels.\n\nFields\n\ntensor: Encoded sequence tensor (e.g. 4D array)\nlabels: Labels (vector or matrix)\nstats: Normalization statistics (only present for training data when normalized)\n\n\n\n\n\n","category":"type"},{"location":"#AutoComputationalGraphTuning.PreprocessedData","page":"Home","title":"AutoComputationalGraphTuning.PreprocessedData","text":"PreprocessedData\n\nContainer for preprocessed train/validation/test splits.\n\nFields\n\ntrain: Training data split\nval: Validation data split\ntest: Test data split\n\n\n\n\n\n","category":"type"},{"location":"#AutoComputationalGraphTuning.ProcessorEvalStats","page":"Home","title":"AutoComputationalGraphTuning.ProcessorEvalStats","text":"Statistics for gradient and processor evaluation\n\n\n\n\n\n","category":"type"},{"location":"#AutoComputationalGraphTuning.TrainingConfig","page":"Home","title":"AutoComputationalGraphTuning.TrainingConfig","text":"TrainingConfig\n\nConfiguration for model training, including all hyperparameters and settings. This struct is used to save/load trial configurations for reproducibility.\n\n\n\n\n\n","category":"type"},{"location":"#AutoComputationalGraphTuning._accumulate_batch_stats!-Union{Tuple{T}, Tuple{Vector, Vector, Vector, Any, Any, Any, Any, Int64, Int64, T}} where T<:Real","page":"Home","title":"AutoComputationalGraphTuning._accumulate_batch_stats!","text":"Accumulate evaluation metrics over batches\n\n\n\n\n\n","category":"method"},{"location":"#AutoComputationalGraphTuning._calculate_r2_with_validation-Union{Tuple{T}, Tuple{AbstractVector{T}, AbstractVector{T}}} where T<:Real","page":"Home","title":"AutoComputationalGraphTuning._calculate_r2_with_validation","text":"Calculate R² for a single row with validation\n\n\n\n\n\n","category":"method"},{"location":"#AutoComputationalGraphTuning._compute_aggregated_r2-Union{Tuple{T}, Tuple{AbstractMatrix{T}, AbstractMatrix{T}, AbstractMatrix{Bool}}} where T<:Real","page":"Home","title":"AutoComputationalGraphTuning._compute_aggregated_r2","text":"Compute aggregated R² across all valid predictions\n\n\n\n\n\n","category":"method"},{"location":"#AutoComputationalGraphTuning._compute_grad_and_preds-Tuple{Any, Any, Any, Int64, Int64}","page":"Home","title":"AutoComputationalGraphTuning._compute_grad_and_preds","text":"Compute gradient and predictions for a code batch\n\n\n\n\n\n","category":"method"},{"location":"#AutoComputationalGraphTuning._compute_individual_r2-Union{Tuple{T}, Tuple{AbstractMatrix{T}, AbstractMatrix{T}, AbstractMatrix{Bool}}} where T<:Real","page":"Home","title":"AutoComputationalGraphTuning._compute_individual_r2","text":"Compute individual R² for each output slots\n\n\n\n\n\n","category":"method"},{"location":"#AutoComputationalGraphTuning._create_final_dataloaders-Tuple{Any, Any, Any}","page":"Home","title":"AutoComputationalGraphTuning._create_final_dataloaders","text":"Create dataloaders for final model training (train+val combined) and testing.\n\nReturns: (dltrain, dltest)\n\n\n\n\n\n","category":"method"},{"location":"#AutoComputationalGraphTuning._prepare_final_model_setup-Tuple{Any, Function}","page":"Home","title":"AutoComputationalGraphTuning._prepare_final_model_setup","text":"Prepare setup for final model training.\n\nReturns: (setup, batch_size)\n\n\n\n\n\n","category":"method"},{"location":"#AutoComputationalGraphTuning._print_eval_results-Tuple{AutoComputationalGraphTuning.ProcessorEvalStats, String}","page":"Home","title":"AutoComputationalGraphTuning._print_eval_results","text":"Print evaluation results in a formatted table\n\n\n\n\n\n","category":"method"},{"location":"#AutoComputationalGraphTuning._print_summary-NTuple{6, Any}","page":"Home","title":"AutoComputationalGraphTuning._print_summary","text":"Print final tuning summary.\n\n\n\n\n\n","category":"method"},{"location":"#AutoComputationalGraphTuning._run_trial-NTuple{13, Any}","page":"Home","title":"AutoComputationalGraphTuning._run_trial","text":"Run a single hyperparameter tuning trial.\n\nReturns: (r2, valloss, numparams, modelstate, model, batchsize)\n\n\n\n\n\n","category":"method"},{"location":"#AutoComputationalGraphTuning._save_if_best!-NTuple{4, Any}","page":"Home","title":"AutoComputationalGraphTuning._save_if_best!","text":"Save results if current trial is best.\n\n\n\n\n\n","category":"method"},{"location":"#AutoComputationalGraphTuning._save_trial_config-NTuple{11, Any}","page":"Home","title":"AutoComputationalGraphTuning._save_trial_config","text":"Save trial configuration to JSON file.\n\n\n\n\n\n","category":"method"},{"location":"#AutoComputationalGraphTuning._setup_save_file-Tuple{Any}","page":"Home","title":"AutoComputationalGraphTuning._setup_save_file","text":"Setup save file path with timestamp.\n\n\n\n\n\n","category":"method"},{"location":"#AutoComputationalGraphTuning._train_code_processor!-Tuple{Any, Any, Function}","page":"Home","title":"AutoComputationalGraphTuning._train_code_processor!","text":"Train code processor using the trained final model and training data.\n\nReturns: (processor, loss_history)\n\n\n\n\n\n","category":"method"},{"location":"#AutoComputationalGraphTuning._train_final_model!-Tuple{Any, Any, Any}","page":"Home","title":"AutoComputationalGraphTuning._train_final_model!","text":"Train the final model and load best weights into model_clone.\n\nReturns: (trained_model, stats)\n\n\n\n\n\n","category":"method"},{"location":"#AutoComputationalGraphTuning._update_best!-NTuple{6, Any}","page":"Home","title":"AutoComputationalGraphTuning._update_best!","text":"Update best model if current trial is better.\n\n\n\n\n\n","category":"method"},{"location":"#AutoComputationalGraphTuning.calculate_r2-Union{Tuple{T}, Tuple{AbstractVector{T}, AbstractVector{T}}} where T<:Real","page":"Home","title":"AutoComputationalGraphTuning.calculate_r2","text":"Calculate R² (coefficient of determination)\n\n\n\n\n\n","category":"method"},{"location":"#AutoComputationalGraphTuning.check_early_stopping!-NTuple{9, Any}","page":"Home","title":"AutoComputationalGraphTuning.check_early_stopping!","text":"Check early stopping condition and update best model state\n\n\n\n\n\n","category":"method"},{"location":"#AutoComputationalGraphTuning.config_to_loss_fcn-Tuple{TrainingConfig}","page":"Home","title":"AutoComputationalGraphTuning.config_to_loss_fcn","text":"config_to_loss_fcn(config::TrainingConfig)\n\nConvert a TrainingConfig's loss function strings back to a named tuple. Assumes the loss function and aggregation are from Flux/StatsBase.\n\n\n\n\n\n","category":"method"},{"location":"#AutoComputationalGraphTuning.count_nonzero-Union{Tuple{AbstractArray{T}}, Tuple{T}, Tuple{AbstractArray{T}, T}} where T<:Real","page":"Home","title":"AutoComputationalGraphTuning.count_nonzero","text":"Count nonzero elements (above threshold)\n\n\n\n\n\n","category":"method"},{"location":"#AutoComputationalGraphTuning.create_masked_loss_function-Tuple{NamedTuple{(:loss, :agg), <:Tuple{var\"#s2\", var\"#s1\"} where {var\"#s2\"<:Function, var\"#s1\"<:Function}}}","page":"Home","title":"AutoComputationalGraphTuning.create_masked_loss_function","text":"create_masked_loss_function(loss_config)\n\nCreate a masked loss function from a configuration named tuple.\n\nArguments\n\nloss_config: Named tuple with loss and agg fields: (loss=loss_function, agg=aggregation_function)\n\nReturns\n\nFunction that computes masked loss: f(predictions, targets, mask)\n\nExamples\n\n# Create MSE loss with mean aggregation\nloss_fn = create_masked_loss_function((loss=Flux.mse, agg=StatsBase.mean))\n\n# Create MAE loss with sum aggregation\nloss_fn = create_masked_loss_function((loss=Flux.mae, agg=sum))\n\n# Use in training\nloss = loss_fn(predictions, targets, mask)\n\n\n\n\n\n","category":"method"},{"location":"#AutoComputationalGraphTuning.evaluate_processor-NTuple{4, Any}","page":"Home","title":"AutoComputationalGraphTuning.evaluate_processor","text":"Evaluate processor performance on a dataset\n\n\n\n\n\n","category":"method"},{"location":"#AutoComputationalGraphTuning.evaluate_validation_loss-Tuple{Any, Any}","page":"Home","title":"AutoComputationalGraphTuning.evaluate_validation_loss","text":"evaluate_validation_loss(model, hp, dataloader)\n\nEvaluate validation loss using masked MSE for NaN handling.\n\nArguments\n\nmodel: Trained model instance\nhp: HyperParameters \ndataloader: Validation data loader\n\nReturns\n\nAverage validation loss over all batches\n\n\n\n\n\n","category":"method"},{"location":"#AutoComputationalGraphTuning.evaluate_validation_metrics-Tuple{Any, Any, Any}","page":"Home","title":"AutoComputationalGraphTuning.evaluate_validation_metrics","text":"evaluate_validation_metrics(model, dataloader, n_outputs)\n\nEvaluate comprehensive validation metrics including loss and R² scores.\n\nArguments\n\nmodel: Trained model instance\ndataloader: Validation data loader  \nn_outputs: Number of output targets (RBPs)\n\nReturns\n\navg_loss: Average validation loss\nindividual_r2: R² score for each RBP\naggregated_r2: Overall R² across all predictions\n\n\n\n\n\n","category":"method"},{"location":"#AutoComputationalGraphTuning.get_function_name-Tuple{Any}","page":"Home","title":"AutoComputationalGraphTuning.get_function_name","text":"get_function_name(f)\n\nGet the fully qualified name of a function, preserving module prefix. For common StatsBase re-exports (like mean), prefer StatsBase module name.\n\n\n\n\n\n","category":"method"},{"location":"#AutoComputationalGraphTuning.get_split_indices-Tuple{Int64}","page":"Home","title":"AutoComputationalGraphTuning.get_split_indices","text":"get_split_indices(data_size::Int; train_ratio=0.8, val_ratio=0.1, test_ratio=0.1, shuffle=true, seed=nothing)\n\nReturn shuffled (or ordered) indices for train/validation/test splits.\n\nArguments\n\ndata_size: Number of data points\ntrain_ratio, val_ratio, test_ratio: Proportions for each split (must sum to 1)\nshuffle: Shuffle indices (default: true)\nseed: Random seed (optional)\n\nReturns\n\nNamed tuple: (train, val, test) index vectors\n\nThrows\n\nArgumentError if ratios are invalid or data_size <= 0\n\nExample\n\ntrainidx, validx, testidx = getsplit_indices(1000; seed=42)\n\n\n\n\n\n","category":"method"},{"location":"#AutoComputationalGraphTuning.leading_colons-Tuple{AbstractArray}","page":"Home","title":"AutoComputationalGraphTuning.leading_colons","text":"leading_colons(x::AbstractArray)\n\nReturn a tuple of : (colons) of length ndims(x) - 1. Useful for slicing all but the last dimension of an array.\n\nExamples\n\nA = rand(3, 4, 5)\nA[leading_colons(A)..., 2]  # selects all elements in the last dimension at index 2\n\n\n\n\n\n","category":"method"},{"location":"#AutoComputationalGraphTuning.load_best_trial_config-Tuple{String}","page":"Home","title":"AutoComputationalGraphTuning.load_best_trial_config","text":"load_best_trial_config(save_folder::String)\n\nLoad the configuration for the best trial (highest R²) from a tuning run. Reads the CSV results file and loads the corresponding JSON config.\n\n\n\n\n\n","category":"method"},{"location":"#AutoComputationalGraphTuning.load_trial_config-Tuple{String}","page":"Home","title":"AutoComputationalGraphTuning.load_trial_config","text":"load_trial_config(json_path::String)\n\nLoad a trial configuration from a JSON file.\n\n\n\n\n\n","category":"method"},{"location":"#AutoComputationalGraphTuning.masked_loss","page":"Home","title":"AutoComputationalGraphTuning.masked_loss","text":"masked_loss(predictions, targets, mask, loss_fcn, agg)\n\nApply any Flux loss function only on valid (masked) entries.\n\nArguments\n\npredictions: Model predictions\ntargets: Ground truth targets  \nmask: Boolean mask indicating valid entries\nloss_fcn: Flux loss function (e.g., Flux.mse, Flux.mae, Flux.huber_loss)\nagg: Aggregation function (default: StatsBase.mean)\n\nReturns\n\nLoss computed only on valid entries specified by mask\n\nExamples\n\n# MSE with mean aggregation (default)\nloss = masked_loss(ŷ, y, mask, Flux.mse, StatsBase.mean)\n\n# MAE with sum aggregation  \nloss = masked_loss(ŷ, y, mask, Flux.mae, sum)\n\n# Huber loss with mean aggregation\nloss = masked_loss(ŷ, y, mask, Flux.huber_loss, StatsBase.mean)\n\n\n\n\n\n","category":"function"},{"location":"#AutoComputationalGraphTuning.print_epoch_summary-NTuple{6, Any}","page":"Home","title":"AutoComputationalGraphTuning.print_epoch_summary","text":"Print epoch summary with training and validation metrics\n\n\n\n\n\n","category":"method"},{"location":"#AutoComputationalGraphTuning.save_trial_config-Tuple{TrainingConfig, String}","page":"Home","title":"AutoComputationalGraphTuning.save_trial_config","text":"save_trial_config(config::TrainingConfig, save_folder::String)\n\nSave a trial configuration to a JSON file in the json subfolder.\n\n\n\n\n\n","category":"method"},{"location":"#AutoComputationalGraphTuning.set_reproducible_seeds!","page":"Home","title":"AutoComputationalGraphTuning.set_reproducible_seeds!","text":"Set all random seeds for reproducible results\n\n\n\n\n\n","category":"function"},{"location":"#AutoComputationalGraphTuning.sparsity_percent-Tuple{Integer, Integer}","page":"Home","title":"AutoComputationalGraphTuning.sparsity_percent","text":"Calculate sparsity percentage\n\n\n\n\n\n","category":"method"},{"location":"#AutoComputationalGraphTuning.train_batch!-NTuple{4, Any}","page":"Home","title":"AutoComputationalGraphTuning.train_batch!","text":"Train single batch with flexible loss computation.\n\nArguments\n\nmodel: The model to train\nopt_state: Optimizer state\nseq, labels: Batch data\ncompute_loss: Function(model, seq, labels, nanmask) -> (loss, auxinfo)\nShould return loss scalar and optionally auxiliary info dict\nDefault: standard masked MSE loss\n\nReturns\n\nloss: Scalar loss value\naux_info: Dict with auxiliary information (e.g., valid_count, regularizers, etc.)\n\n\n\n\n\n","category":"method"},{"location":"#AutoComputationalGraphTuning.train_code_processor-Tuple{Any, Any, Any}","page":"Home","title":"AutoComputationalGraphTuning.train_code_processor","text":"Train a code processor to learn gradient transformations.\n\nArguments\n\nmodel: Pre-trained model with code processor architecture\ndataloader: DataLoader with training data\nproc_wrap: Named tuple containing processor functions and config with fields:\ncreate_processor: Function to create processor\narch_type: Architecture type (e.g., :plain, :mbconv)\npredict_from_code: Function to predict from code\nprocess_code: Function to process code with gradient\nseed: Random seed (default: 42)\nmax_epochs: Training epochs (default: 15)\npredict_position: Position for prediction (default: 1)\nuse_hard_mask: Whether to use hard masking in processor (default: true)\ninference_code_layer: Layer for code inference (default: from model.hp)\n\nReturns\n\nprocessor: Trained code processor\nloss_history: Training loss per epoch\n\nExample\n\n# Define processor wrapper\nproc_wrap = (\n    create_processor = create_code_processor,\n    arch_type = :mbconv,\n    predict_from_code = predict_from_code,\n    process_code = process_code\n)\n\n# Train final model first\nmodel, stats, train_stats, dl_train, dl_test = train_final_model(data, create_model; seed=42)\n\n# Then train processor\nprocessor, losses = train_code_processor(\n    model, dl_train, proc_wrap;\n    seed=42,\n    max_epochs=20,\n    use_hard_mask=true\n)\n\n\n\n\n\n","category":"method"},{"location":"#AutoComputationalGraphTuning.train_epoch!-NTuple{5, Any}","page":"Home","title":"AutoComputationalGraphTuning.train_epoch!","text":"Train single epoch\n\n\n\n\n\n","category":"method"},{"location":"#AutoComputationalGraphTuning.train_final_model-Tuple{Any, Function}","page":"Home","title":"AutoComputationalGraphTuning.train_final_model","text":"Train final model using combined train+val data, evaluate on test set.\n\nArguments\n\nraw_data: Raw input data\ncreate_model: Function to create the model\nseed: Random seed (default: 1)\nmax_epochs: Maximum training epochs for final model (default: 50)\npatience: Early stopping patience (default: 10)\nprint_every: Print frequency (default: 100)\nrandomize_batchsize: Whether to randomize batch size (default: true)\nnormalize_Y: Whether to normalize labels (default: true)\nnormalization_method: Normalization method (default: :zscore)\nnormalization_mode: Normalization mode (default: :rowwise)\nuse_cuda: Whether to use CUDA (default: true)\nloss_fcn: Loss function specification (default: (loss=Flux.mse, agg=StatsBase.mean))\nmodel_kwargs...: Additional model keyword arguments\n\nReturns\n\n(model, stats, train_stats, dl_train, dl_test)\n\nmodel: Trained final model\nstats: Training statistics\ntrain_stats: Training setup statistics\ndl_train: Training dataloader (for optional processor training)\ndl_test: Test dataloader (for optional processor evaluation)\n\nExamples\n\n# Train final model and get dataloaders\nmodel, stats, train_stats, dl_train, dl_test = train_final_model(data, create_model; seed=42)\n\n# Then optionally train processor using the same dataloaders\nprocessor, proc_losses = train_code_processor(\n    model, dl_train, MyModule.create_code_processor;\n    arch_type=:mbconv,\n    process_code_fn=MyModule.process_code_with_gradient,\n    predict_from_code_fn=MyModule.predict_from_code\n)\n\n\n\n\n\n","category":"method"},{"location":"#AutoComputationalGraphTuning.train_final_model_from_config-Tuple{Any, Function, TrainingConfig}","page":"Home","title":"AutoComputationalGraphTuning.train_final_model_from_config","text":"Train final model from saved config (e.g., best trial from tuning).\n\nArguments\n\nraw_data: Raw input data\ncreate_model: Function to create the model\nconfig: TrainingConfig from saved trial\nmax_epochs: Maximum training epochs (default: 50)\npatience: Early stopping patience (default: 10)\nprint_every: Print frequency (default: 100)\nmodel_kwargs...: Additional model keyword arguments\n\nReturns\n\n(model, stats, train_stats, dl_train, dl_test)\n\n\n\n\n\n","category":"method"},{"location":"#AutoComputationalGraphTuning.train_model-NTuple{5, Any}","page":"Home","title":"AutoComputationalGraphTuning.train_model","text":"Train a model with early stopping and return the best model state and training stats.\n\nArguments\n\ncompute_loss: Optional custom loss function(model, seq, labels, nanmask) -> (loss, auxinfo)\nIf not provided, uses standard masked MSE\nCan return auxiliary info dict for logging custom metrics\nExamples: gradient penalties, attention regularization, multi-task losses\n\nReturns\n\nbest_model_state: State dict of the best model (lowest validation loss)\ntraining_stats: Dict with training history and final metrics\n\nExample Custom Loss\n\n# Gradient penalty example\nfunction my_loss(model, seq, labels, mask)\n    # Forward pass (model can return multiple outputs)\n    output = model(seq)\n    preds = output isa Tuple ? output[1] : output\n    \n    # Standard prediction loss\n    pred_loss = masked_mse(preds, labels, mask)\n    \n    # Custom regularizer (e.g., gradient penalty)\n    grad_penalty = compute_gradient_penalty(model, seq)\n    \n    total_loss = pred_loss + 0.1 * grad_penalty\n    aux = Dict(:pred_loss => pred_loss, :grad_penalty => grad_penalty)\n    \n    (total_loss, aux)\nend\n\n# Use it\ntrain_model(model, opt, train_dl, val_dl, ydim; compute_loss=my_loss)\n\n\n\n\n\n","category":"method"},{"location":"#AutoComputationalGraphTuning.train_val_test_split-Tuple{Any}","page":"Home","title":"AutoComputationalGraphTuning.train_val_test_split","text":"train_val_test_split(data, labels; train_ratio=0.8, val_ratio=0.1, test_ratio=0.1, shuffle=true, seed=nothing)\n\nSplit data and labels together into train/validation/test sets. Uses @views for memory efficiency - returns lightweight views instead of copying data.\n\nArguments\n\ndata: NamedTuple with fields X (features) and Y (labels)\ntrain_ratio, val_ratio, test_ratio: Proportions for each split (must sum to 1)\n_shuffle: Shuffle indices (default: true)\nrng: Random number generator (default: Random.GLOBAL_RNG)\n\nReturns\n\nNamed tuple with (train=(X=..., Y=...), val=(...), test=(...))\nAll data and labels are views (SubArrays) for memory efficiency\n\nExamples\n\ndata = (X = rand(10, 100), Y = rand(1, 100))\nsplits = train_val_test_split(data; seed=42)\ntrain_X = splits.train.X\ntrain_Y = splits.train.Y\n\n\n\n\n\n","category":"method"},{"location":"#AutoComputationalGraphTuning.tune_hyperparameters-Tuple{Any, Function}","page":"Home","title":"AutoComputationalGraphTuning.tune_hyperparameters","text":"Tune hyperparameters across multiple trials with different seeds.\n\nReturns: (resultsdf, bestmodel, best_info)\n\nresults_df: DataFrame with all trial results\nbest_model: Model with highest validation R²\nbestinfo: NamedTuple (seed, r2, batchsize) of best trial\n\n\n\n\n\n","category":"method"}]
}
