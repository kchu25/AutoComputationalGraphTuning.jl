var documenterSearchIndex = {"docs":
[{"location":"#AutoComputationalGraphTuning","page":"Home","title":"AutoComputationalGraphTuning","text":"Documentation for AutoComputationalGraphTuning.","category":"section"},{"location":"#Quick-Start","page":"Home","title":"Quick Start","text":"This package makes hyperparameter tuning for Flux models simple. You just need a data struct and a model creation function.","category":"section"},{"location":"#What-You-Need","page":"Home","title":"What You Need","text":"Data struct with four fields:\n\ndata.X, data.Y - your features and labels\ndata.X_dim, data.Y_dim - dimensions of each feature/label\n\nModel function that creates a Flux model:\n\nfunction create_model(X_dim, Y_dim, batch_size; rng, use_cuda)\n    # Your model code here\n    return model  # Must have a `linear_sum` property\nend","category":"section"},{"location":"#Loss-Function-Flexibility","page":"Home","title":"Loss Function Flexibility","text":"Want to experiment with different loss functions? Easy:\n\n# Default MSE\ntune_hyperparameters(data, create_model)\n\n# Try MAE instead (less sensitive to outliers)\ntune_hyperparameters(data, create_model; \n                    loss_fcn=(loss=Flux.mae, agg=StatsBase.mean))\n\n# Huber loss (robust choice)\ntune_hyperparameters(data, create_model;\n                    loss_fcn=(loss=Flux.huber_loss, agg=StatsBase.mean))\n\nThe loss_fcn parameter takes any Flux loss function with your choice of aggregation. Works the same way in train_final_model() too.","category":"section"},{"location":"#Basic-Workflow","page":"Home","title":"Basic Workflow","text":"# 1. Tune hyperparameters\nresults = tune_hyperparameters(data, create_model; \n                              max_epochs=50, n_trials=100)\n\n# 2. Train final model  \nmodel, stats = train_final_model(data, create_model; \n                                seed=42, max_epochs=100)\n\nThe package handles data splitting, normalization, early stopping, and model selection automatically.\n\n","category":"section"},{"location":"#AutoComputationalGraphTuning.DataSplit","page":"Home","title":"AutoComputationalGraphTuning.DataSplit","text":"DataSplit\n\nRepresents a single data split with tensor and labels.\n\nFields\n\ntensor: Encoded sequence tensor (e.g. 4D array)\nlabels: Labels (vector or matrix)\nstats: Normalization statistics (only present for training data when normalized)\n\n\n\n\n\n","category":"type"},{"location":"#AutoComputationalGraphTuning.PreprocessedData","page":"Home","title":"AutoComputationalGraphTuning.PreprocessedData","text":"PreprocessedData\n\nContainer for preprocessed train/validation/test splits.\n\nFields\n\ntrain: Training data split\nval: Validation data split\ntest: Test data split\n\n\n\n\n\n","category":"type"},{"location":"#AutoComputationalGraphTuning.ProcessorEvalStats","page":"Home","title":"AutoComputationalGraphTuning.ProcessorEvalStats","text":"Statistics for processor evaluation\n\n\n\n\n\n","category":"type"},{"location":"#AutoComputationalGraphTuning.ThresholdEvalStats","page":"Home","title":"AutoComputationalGraphTuning.ThresholdEvalStats","text":"Statistics for threshold evaluation\n\n\n\n\n\n","category":"type"},{"location":"#AutoComputationalGraphTuning.TrainingConfig","page":"Home","title":"AutoComputationalGraphTuning.TrainingConfig","text":"TrainingConfig\n\nConfiguration for model training, including all hyperparameters and settings. This struct is used to save/load trial configurations for reproducibility.\n\n\n\n\n\n","category":"type"},{"location":"#AutoComputationalGraphTuning._compute_code_gradient_and_linear_output-Tuple{Any, Any, Int64}","page":"Home","title":"AutoComputationalGraphTuning._compute_code_gradient_and_linear_output","text":"Compute code gradient and linear (pre-activation) output for a single batch\n\n\n\n\n\n","category":"method"},{"location":"#AutoComputationalGraphTuning._compute_code_gradients-Tuple{Any, Any, Any, Int64, Int64}","page":"Home","title":"AutoComputationalGraphTuning._compute_code_gradients","text":"Compute predictions and gradients w.r.t. code\n\n\n\n\n\n","category":"method"},{"location":"#AutoComputationalGraphTuning._compute_r2-Union{Tuple{T}, Tuple{AbstractVector{T}, AbstractVector{T}}} where T<:AbstractFloat","page":"Home","title":"AutoComputationalGraphTuning._compute_r2","text":"Compute R² coefficient, excluding NaN values\n\n\n\n\n\n","category":"method"},{"location":"#AutoComputationalGraphTuning._create_eval_dataloaders-Tuple{Any, Any}","page":"Home","title":"AutoComputationalGraphTuning._create_eval_dataloaders","text":"Create eval dataloaders with no shuffling and partial=true so every sample is included and order matches splitindices. Returns: (dltraineval, dltest_eval)\n\n\n\n\n\n","category":"method"},{"location":"#AutoComputationalGraphTuning._create_final_dataloaders-Tuple{Any, Any, Any}","page":"Home","title":"AutoComputationalGraphTuning._create_final_dataloaders","text":"Create dataloaders for final model training. Returns: (dltrain, dltest)\n\n\n\n\n\n","category":"method"},{"location":"#AutoComputationalGraphTuning._evaluate_with_threshold-Union{Tuple{T}, Tuple{Any, Any, Any, T, Int64}} where T<:AbstractFloat","page":"Home","title":"AutoComputationalGraphTuning._evaluate_with_threshold","text":"Evaluate processor performance with a specific threshold applied to proc_gyro·code products.\n\nArguments\n\nmodel: Trained model\nprocessor: Trained code processor\ndataloader: DataLoader for evaluation\nthreshold: Magnitude threshold for masking proc_gyro·code product components\npredict_position: Position for prediction\n\nReturns\n\nThresholdEvalStats containing R² scores and sparsity metrics\n\n\n\n\n\n","category":"method"},{"location":"#AutoComputationalGraphTuning._init_processor-Tuple{Any, Any, Bool, Int64}","page":"Home","title":"AutoComputationalGraphTuning._init_processor","text":"Initialize processor and optimizer with seeded RNG\n\n\n\n\n\n","category":"method"},{"location":"#AutoComputationalGraphTuning._prepare_final_model_setup-Tuple{Any, Function}","page":"Home","title":"AutoComputationalGraphTuning._prepare_final_model_setup","text":"Prepare setup for final model training. Returns: (setup, batch_size)\n\n\n\n\n\n","category":"method"},{"location":"#AutoComputationalGraphTuning._print_summary-NTuple{6, Any}","page":"Home","title":"AutoComputationalGraphTuning._print_summary","text":"Print final tuning summary.\n\n\n\n\n\n","category":"method"},{"location":"#AutoComputationalGraphTuning._processor_train_step!-Tuple{Any, Any, Any, Any, Any, Int64, Int64, Int64}","page":"Home","title":"AutoComputationalGraphTuning._processor_train_step!","text":"Single training step for code processor\n\n\n\n\n\n","category":"method"},{"location":"#AutoComputationalGraphTuning._run_trial-NTuple{13, Any}","page":"Home","title":"AutoComputationalGraphTuning._run_trial","text":"Run a single hyperparameter tuning trial.\n\nReturns: (r2, valloss, numparams, modelstate, model, batchsize)\n\n\n\n\n\n","category":"method"},{"location":"#AutoComputationalGraphTuning._save_if_best!-NTuple{4, Any}","page":"Home","title":"AutoComputationalGraphTuning._save_if_best!","text":"Save results if current trial is best.\n\n\n\n\n\n","category":"method"},{"location":"#AutoComputationalGraphTuning._save_trial_config-NTuple{11, Any}","page":"Home","title":"AutoComputationalGraphTuning._save_trial_config","text":"Save trial configuration to JSON file.\n\n\n\n\n\n","category":"method"},{"location":"#AutoComputationalGraphTuning._setup_save_file-Tuple{Any}","page":"Home","title":"AutoComputationalGraphTuning._setup_save_file","text":"Setup save file path with timestamp.\n\n\n\n\n\n","category":"method"},{"location":"#AutoComputationalGraphTuning._train_final_model!-Tuple{Any, Any, Any}","page":"Home","title":"AutoComputationalGraphTuning._train_final_model!","text":"Train final model and load best weights. Returns: (trained_model, stats)\n\n\n\n\n\n","category":"method"},{"location":"#AutoComputationalGraphTuning._train_processor_epoch!-Tuple{Any, Any, Any, Any, Any, Int64, Int64, Ref{Int64}}","page":"Home","title":"AutoComputationalGraphTuning._train_processor_epoch!","text":"Train epoch for code processor\n\n\n\n\n\n","category":"method"},{"location":"#AutoComputationalGraphTuning._update_best!-NTuple{6, Any}","page":"Home","title":"AutoComputationalGraphTuning._update_best!","text":"Update best model if current trial is better.\n\n\n\n\n\n","category":"method"},{"location":"#AutoComputationalGraphTuning.check_early_stopping!-NTuple{9, Any}","page":"Home","title":"AutoComputationalGraphTuning.check_early_stopping!","text":"Check early stopping condition and update best model state\n\n\n\n\n\n","category":"method"},{"location":"#AutoComputationalGraphTuning.compile_loss-Tuple{NamedTuple{(:loss, :agg), <:Tuple{var\"#s2\", var\"#s1\"} where {var\"#s2\"<:Function, var\"#s1\"<:Function}}}","page":"Home","title":"AutoComputationalGraphTuning.compile_loss","text":"compile_loss(loss_spec) -> compiled_loss(predictions, targets, mask)\n\nCompile a loss specification (NamedTuple) into a callable 3-arg loss function.\n\nArguments\n\nloss_spec: Named tuple (loss=loss_function, agg=aggregation_function)\n\nReturns\n\ncompiled_loss: Function (predictions, targets, mask) -> scalar\n\nExamples\n\n# Compile MSE loss with mean aggregation\ncompiled = compile_loss((loss=Flux.mse, agg=StatsBase.mean))\n\n# Compile MAE loss with sum aggregation\ncompiled = compile_loss((loss=Flux.mae, agg=sum))\n\n# Use in training\nloss = compiled(predictions, targets, mask)\n\n\n\n\n\n","category":"method"},{"location":"#AutoComputationalGraphTuning.compute_r2_scores-Tuple{Any, Any, Any}","page":"Home","title":"AutoComputationalGraphTuning.compute_r2_scores","text":"compute_r2_scores(predictions, targets, mask)\n\nCompute R² scores for individual RBPs and aggregated across all predictions.\n\nArguments\n\npredictions: Model predictions matrix (nrbps, nsamples)\ntargets: Ground truth targets matrix (nrbps, nsamples)  \nmask: Boolean mask for valid entries (nrbps, nsamples)\n\nReturns\n\nindividual_r2: R² score for each RBP (may contain NaN for insufficient data)\naggregated_r2: Overall R² across all valid predictions\n\n\n\n\n\n","category":"method"},{"location":"#AutoComputationalGraphTuning.evaluate_processor-Tuple{Any, Any, Any, String}","page":"Home","title":"AutoComputationalGraphTuning.evaluate_processor","text":"Evaluate code processor performance on a dataset.\n\nArguments\n\nmodel: Trained model\nprocessor: Trained code processor\ndataloader: DataLoader for evaluation\nset_name: Name of the dataset (e.g., \"Train\", \"Test\")\npredict_position: Position for prediction (default: 1)\n\nReturns\n\nProcessorEvalStats containing R² scores for original gradients vs processor gradients\n\nExample\n\nstats_train = evaluate_processor(m, processor, dl_train, \"Train\")\nstats_test = evaluate_processor(m, processor, dl_test, \"Test\")\n\n\n\n\n\n","category":"method"},{"location":"#AutoComputationalGraphTuning.evaluate_validation_loss-Tuple{Any, Any}","page":"Home","title":"AutoComputationalGraphTuning.evaluate_validation_loss","text":"evaluate_validation_loss(model, hp, dataloader)\n\nEvaluate validation loss using masked MSE for NaN handling.\n\nArguments\n\nmodel: Trained model instance\nhp: HyperParameters \ndataloader: Validation data loader\n\nReturns\n\nAverage validation loss over all batches\n\n\n\n\n\n","category":"method"},{"location":"#AutoComputationalGraphTuning.evaluate_validation_metrics-Tuple{Any, Any, Any}","page":"Home","title":"AutoComputationalGraphTuning.evaluate_validation_metrics","text":"evaluate_validation_metrics(model, dataloader, n_outputs)\n\nEvaluate comprehensive validation metrics including loss and R² scores.\n\nArguments\n\nmodel: Trained model instance\ndataloader: Validation data loader  \nn_outputs: Number of output targets (RBPs)\n\nReturns\n\navg_loss: Average validation loss\nindividual_r2: R² score for each RBP\naggregated_r2: Overall R² across all predictions\n\n\n\n\n\n","category":"method"},{"location":"#AutoComputationalGraphTuning.find_optimal_threshold-Union{Tuple{T}, Tuple{Any, Any, Any, Any, AutoComputationalGraphTuning.ProcessorEvalStats{T}}} where T<:AbstractFloat","page":"Home","title":"AutoComputationalGraphTuning.find_optimal_threshold","text":"Find optimal threshold for proc_gyro that maximizes sparsity while maintaining R² performance.\n\nSearches for the highest threshold where masking proc_gyro·code products below threshold still maintains acceptable R² on the test set compared to the baseline processor R².\n\nArguments\n\nmodel: Trained model\nprocessor: Trained code processor\ndataloader_train: Training data loader (for finding threshold)\ndataloader_test: Test data loader (for evaluating R²)\nbaseline_stats: ProcessorEvalStats from evaluateprocessor (contains baseline r2processor)\nr2_tolerance: Maximum acceptable R² drop (default: 0.05, meaning max 5% relative drop)\nnum_candidates: Number of threshold values to test (default: 20)\npredict_position: Position for prediction (default: 1)\n\nReturns\n\nThresholdEvalStats containing optimal threshold and performance metrics\n\nExample\n\n# First get baseline performance\nbaseline = evaluate_processor(m, processor, dl_test, \"Test\")\n# Then find optimal threshold\nthresh_stats = find_optimal_threshold(m, processor, dl_train, dl_test, baseline)\n\n\n\n\n\n","category":"method"},{"location":"#AutoComputationalGraphTuning.get_function_name-Tuple{Any}","page":"Home","title":"AutoComputationalGraphTuning.get_function_name","text":"get_function_name(f)\n\nGet the fully qualified name of a function, preserving module prefix. For common StatsBase re-exports (like mean), prefer StatsBase module name.\n\n\n\n\n\n","category":"method"},{"location":"#AutoComputationalGraphTuning.get_split_indices-Tuple{Int64}","page":"Home","title":"AutoComputationalGraphTuning.get_split_indices","text":"get_split_indices(data_size::Int; train_ratio=0.8, val_ratio=0.1, test_ratio=0.1, shuffle=true, seed=nothing)\n\nReturn shuffled (or ordered) indices for train/validation/test splits.\n\nArguments\n\ndata_size: Number of data points\ntrain_ratio, val_ratio, test_ratio: Proportions for each split (must sum to 1)\nshuffle: Shuffle indices (default: true)\nseed: Random seed (optional)\n\nReturns\n\nNamed tuple: (train, val, test) index vectors\n\nThrows\n\nArgumentError if ratios are invalid or data_size <= 0\n\nExample\n\ntrainidx, validx, testidx = getsplit_indices(1000; seed=42)\n\n\n\n\n\n","category":"method"},{"location":"#AutoComputationalGraphTuning.leading_colons-Tuple{AbstractArray}","page":"Home","title":"AutoComputationalGraphTuning.leading_colons","text":"leading_colons(x::AbstractArray)\n\nReturn a tuple of : (colons) of length ndims(x) - 1. Useful for slicing all but the last dimension of an array.\n\nExamples\n\nA = rand(3, 4, 5)\nA[leading_colons(A)..., 2]  # selects all elements in the last dimension at index 2\n\n\n\n\n\n","category":"method"},{"location":"#AutoComputationalGraphTuning.load_best_trial_config-Tuple{String}","page":"Home","title":"AutoComputationalGraphTuning.load_best_trial_config","text":"load_best_trial_config(save_folder::String)\n\nLoad the configuration for the best trial (highest R²) from a tuning run. Reads the CSV results file and loads the corresponding JSON config.\n\n\n\n\n\n","category":"method"},{"location":"#AutoComputationalGraphTuning.load_trial_config-Tuple{String}","page":"Home","title":"AutoComputationalGraphTuning.load_trial_config","text":"load_trial_config(json_path::String)\n\nLoad a trial configuration from a JSON file.\n\n\n\n\n\n","category":"method"},{"location":"#AutoComputationalGraphTuning.masked_loss","page":"Home","title":"AutoComputationalGraphTuning.masked_loss","text":"masked_loss(predictions, targets, mask, loss_fcn, agg)\n\nApply any Flux loss function only on valid (masked) entries.\n\nArguments\n\npredictions: Model predictions\ntargets: Ground truth targets  \nmask: Boolean mask indicating valid entries\nloss_fcn: Flux loss function (e.g., Flux.mse, Flux.mae, Flux.huber_loss)\nagg: Aggregation function (default: StatsBase.mean)\n\nReturns\n\nLoss computed only on valid entries specified by mask\n\nExamples\n\n# MSE with mean aggregation (default)\nloss = masked_loss(ŷ, y, mask, Flux.mse, StatsBase.mean)\n\n# MAE with sum aggregation  \nloss = masked_loss(ŷ, y, mask, Flux.mae, sum)\n\n# Huber loss with mean aggregation\nloss = masked_loss(ŷ, y, mask, Flux.huber_loss, StatsBase.mean)\n\n\n\n\n\n","category":"function"},{"location":"#AutoComputationalGraphTuning.print_epoch_summary-NTuple{6, Any}","page":"Home","title":"AutoComputationalGraphTuning.print_epoch_summary","text":"Print epoch summary with training and validation metrics\n\n\n\n\n\n","category":"method"},{"location":"#AutoComputationalGraphTuning.save_trial_config-Tuple{TrainingConfig, String}","page":"Home","title":"AutoComputationalGraphTuning.save_trial_config","text":"save_trial_config(config::TrainingConfig, save_folder::String)\n\nSave a trial configuration to a JSON file in the json subfolder.\n\n\n\n\n\n","category":"method"},{"location":"#AutoComputationalGraphTuning.set_reproducible_seeds!","page":"Home","title":"AutoComputationalGraphTuning.set_reproducible_seeds!","text":"Set all random seeds for reproducible results\n\n\n\n\n\n","category":"function"},{"location":"#AutoComputationalGraphTuning.train_batch!-NTuple{4, Any}","page":"Home","title":"AutoComputationalGraphTuning.train_batch!","text":"Train single batch with flexible loss computation.\n\nArguments\n\nmodel: The model to train\nopt_state: Optimizer state\nseq, labels: Batch data\ncompute_loss: Function(model, seq, labels, nanmask) -> (loss, auxinfo)\nShould return loss scalar and optionally auxiliary info dict\nDefault: standard masked MSE loss\n\nReturns\n\nloss: Scalar loss value\naux_info: Dict with auxiliary information (e.g., valid_count, regularizers, etc.)\n\n\n\n\n\n","category":"method"},{"location":"#AutoComputationalGraphTuning.train_code_processor-Tuple{Any, Any, Any}","page":"Home","title":"AutoComputationalGraphTuning.train_code_processor","text":"Train a code processor to learn gradient transformations.\n\nArguments\n\nmodel: Pre-trained model (frozen during training)\ndataloader: Training DataLoader\nproc_wrap: Named tuple with (createprocessor, archtype, predictfromcode, process_code)\nseed: Random seed (default: 42)\nmax_epochs: Training epochs (default: 20)\npredict_position: Position for prediction (default: 1)\nuse_hard_mask: Use hard masking in processor (default: true)\ninference_code_layer: Layer for code inference (default: from model.hp)\n\nReturns\n\nprocessor: Trained code processor\nloss_history: Training loss per epoch\n\nExample\n\nproc_wrap = (create_processor=create_code_processor, arch_type=:mbconv,\n             predict_from_code=predict_from_code, process_code=process_code)\nmodel, _, _, dl_train, _ = train_final_model(data, create_model; seed=42)\nprocessor, losses = train_code_processor(model, dl_train, proc_wrap; max_epochs=20)\n\n\n\n\n\n","category":"method"},{"location":"#AutoComputationalGraphTuning.train_epoch!-NTuple{5, Any}","page":"Home","title":"AutoComputationalGraphTuning.train_epoch!","text":"Train single epoch\n\n\n\n\n\n","category":"method"},{"location":"#AutoComputationalGraphTuning.train_final_model-Tuple{Any, Function}","page":"Home","title":"AutoComputationalGraphTuning.train_final_model","text":"Train final model using combined train+val data, evaluate on test set.\n\nArguments\n\nraw_data: Raw input data\ncreate_model: Function to create the model\nseed, max_epochs, patience, print_every: Training hyperparameters\nrandomize_batchsize, normalize_Y, normalization_method, normalization_mode: Data config\nuse_cuda, loss_spec: Compute settings\nmodel_kwargs...: Additional model arguments\n\nReturns\n\n(model, stats, train_stats, dl_train, dl_test) where dataloaders can be reused for processor training\n\nExample\n\nmodel, stats, train_stats, dl_train, dl_test = train_final_model(data, create_model; seed=42)\nprocessor, _ = train_code_processor(model, dl_train, proc_wrap)\n\n\n\n\n\n","category":"method"},{"location":"#AutoComputationalGraphTuning.train_final_model_from_config-Tuple{Any, Function, TrainingConfig, Any}","page":"Home","title":"AutoComputationalGraphTuning.train_final_model_from_config","text":"Train final model from saved config (e.g., best trial from tuning).\n\nReturns\n\n(model, stats, train_stats, dl_train, dl_test)\n\n\n\n\n\n","category":"method"},{"location":"#AutoComputationalGraphTuning.train_model-NTuple{5, Any}","page":"Home","title":"AutoComputationalGraphTuning.train_model","text":"Train a model with early stopping and return the best model state and training stats.\n\nArguments\n\ncompute_loss: Optional custom loss function(model, seq, labels, nanmask) -> (loss, auxinfo)\nIf not provided, uses standard masked MSE\nCan return auxiliary info dict for logging custom metrics\nExamples: gradient penalties, attention regularization, multi-task losses\n\nReturns\n\nbest_model_state: State dict of the best model (lowest validation loss)\ntraining_stats: Dict with training history and final metrics\n\nExample Custom Loss\n\n# Gradient penalty example\nfunction my_loss(model, seq, labels, mask)\n    # Forward pass (model can return multiple outputs)\n    output = model(seq)\n    preds = output isa Tuple ? output[1] : output\n    \n    # Standard prediction loss\n    pred_loss = masked_mse(preds, labels, mask)\n    \n    # Custom regularizer (e.g., gradient penalty)\n    grad_penalty = compute_gradient_penalty(model, seq)\n    \n    total_loss = pred_loss + 0.1 * grad_penalty\n    aux = Dict(:pred_loss => pred_loss, :grad_penalty => grad_penalty)\n    \n    (total_loss, aux)\nend\n\n# Use it\ntrain_model(model, opt, train_dl, val_dl, ydim; compute_loss=my_loss)\n\n# Or use a compiled loss (3-arg: preds, labels, mask) directly:\ntrain_model(model, opt, train_dl, val_dl, ydim; compiled_loss=my_compiled_loss)\n\n\n\n\n\n","category":"method"},{"location":"#AutoComputationalGraphTuning.train_val_test_split-Tuple{Any}","page":"Home","title":"AutoComputationalGraphTuning.train_val_test_split","text":"train_val_test_split(data, labels; train_ratio=0.8, val_ratio=0.1, test_ratio=0.1, shuffle=true, seed=nothing)\n\nSplit data and labels together into train/validation/test sets. Uses @views for memory efficiency - returns lightweight views instead of copying data.\n\nArguments\n\ndata: NamedTuple with fields X (features) and Y (labels)\ntrain_ratio, val_ratio, test_ratio: Proportions for each split (must sum to 1)\n_shuffle: Shuffle indices (default: true)\nrng: Random number generator (default: Random.GLOBAL_RNG)\n\nReturns\n\nNamed tuple with (train=(X=..., Y=...), val=(...), test=(...))\nAll data and labels are views (SubArrays) for memory efficiency\n\nExamples\n\ndata = (X = rand(10, 100), Y = rand(1, 100))\nsplits, splits_indices = train_val_test_split(data; seed=42)\ntrain_X = splits.train.X\ntrain_Y = splits.train.Y\n\n\n\n\n\n","category":"method"},{"location":"#AutoComputationalGraphTuning.tune_hyperparameters-Tuple{Any, Function}","page":"Home","title":"AutoComputationalGraphTuning.tune_hyperparameters","text":"Tune hyperparameters across multiple trials with different seeds.\n\nReturns: (resultsdf, bestmodel, best_info)\n\nresults_df: DataFrame with all trial results\nbest_model: Model with highest validation R²\nbestinfo: NamedTuple (seed, r2, batchsize) of best trial\n\n\n\n\n\n","category":"method"}]
}
