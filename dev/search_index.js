var documenterSearchIndex = {"docs":
[{"location":"#AutoComputationalGraphTuning","page":"Home","title":"AutoComputationalGraphTuning","text":"Documentation for AutoComputationalGraphTuning.","category":"section"},{"location":"#Quick-Start","page":"Home","title":"Quick Start","text":"This package makes hyperparameter tuning for Flux models simple. You just need a data struct and a model creation function.","category":"section"},{"location":"#What-You-Need","page":"Home","title":"What You Need","text":"Data struct with four fields:\n\ndata.X, data.Y - your features and labels\ndata.X_dim, data.Y_dim - dimensions of each feature/label\n\nModel function that creates a Flux model:\n\nfunction create_model(X_dim, Y_dim, batch_size; rng, use_cuda)\n    # Your model code here\n    return model  # Must have a `linear_sum` property\nend","category":"section"},{"location":"#Loss-Function-Flexibility","page":"Home","title":"Loss Function Flexibility","text":"Want to experiment with different loss functions? Easy:\n\n# Default MSE\ntune_hyperparameters(data, create_model)\n\n# Try MAE instead (less sensitive to outliers)\ntune_hyperparameters(data, create_model; \n                    loss_fcn=(loss=Flux.mae, agg=StatsBase.mean))\n\n# Huber loss (robust choice)\ntune_hyperparameters(data, create_model;\n                    loss_fcn=(loss=Flux.huber_loss, agg=StatsBase.mean))\n\nThe loss_fcn parameter takes any Flux loss function with your choice of aggregation. Works the same way in train_final_model() too.","category":"section"},{"location":"#Basic-Workflow","page":"Home","title":"Basic Workflow","text":"# 1. Tune hyperparameters\nresults = tune_hyperparameters(data, create_model; \n                              max_epochs=50, n_trials=100)\n\n# 2. Train final model  \nmodel, stats = train_final_model(data, create_model; \n                                seed=42, max_epochs=100)\n\nThe package handles data splitting, normalization, early stopping, and model selection automatically.\n\n","category":"section"},{"location":"#AutoComputationalGraphTuning.DataSplit","page":"Home","title":"AutoComputationalGraphTuning.DataSplit","text":"DataSplit\n\nRepresents a single data split with tensor and labels.\n\nFields\n\ntensor: Encoded sequence tensor (e.g. 4D array)\nlabels: Labels (vector or matrix)\nstats: Normalization statistics (only present for training data when normalized)\n\n\n\n\n\n","category":"type"},{"location":"#AutoComputationalGraphTuning.PreprocessedData","page":"Home","title":"AutoComputationalGraphTuning.PreprocessedData","text":"PreprocessedData\n\nContainer for preprocessed train/validation/test splits.\n\nFields\n\ntrain: Training data split\nval: Validation data split\ntest: Test data split\n\n\n\n\n\n","category":"type"},{"location":"#AutoComputationalGraphTuning.TrainingConfig","page":"Home","title":"AutoComputationalGraphTuning.TrainingConfig","text":"TrainingConfig\n\nConfiguration for model training, including all hyperparameters and settings. This struct is used to save/load trial configurations for reproducibility.\n\n\n\n\n\n","category":"type"},{"location":"#AutoComputationalGraphTuning.check_early_stopping!-NTuple{9, Any}","page":"Home","title":"AutoComputationalGraphTuning.check_early_stopping!","text":"Check early stopping condition and update best model state\n\n\n\n\n\n","category":"method"},{"location":"#AutoComputationalGraphTuning.compute_r2_scores-Tuple{Any, Any, Any}","page":"Home","title":"AutoComputationalGraphTuning.compute_r2_scores","text":"compute_r2_scores(predictions, targets, mask)\n\nCompute R² scores for individual RBPs and aggregated across all predictions.\n\nArguments\n\npredictions: Model predictions matrix (nrbps, nsamples)\ntargets: Ground truth targets matrix (nrbps, nsamples)  \nmask: Boolean mask for valid entries (nrbps, nsamples)\n\nReturns\n\nindividual_r2: R² score for each RBP (may contain NaN for insufficient data)\naggregated_r2: Overall R² across all valid predictions\n\n\n\n\n\n","category":"method"},{"location":"#AutoComputationalGraphTuning.config_to_loss_fcn-Tuple{TrainingConfig}","page":"Home","title":"AutoComputationalGraphTuning.config_to_loss_fcn","text":"config_to_loss_fcn(config::TrainingConfig)\n\nConvert a TrainingConfig's loss function strings back to a named tuple. Assumes the loss function and aggregation are from Flux/StatsBase.\n\n\n\n\n\n","category":"method"},{"location":"#AutoComputationalGraphTuning.create_masked_loss_function-Tuple{NamedTuple{(:loss, :agg), <:Tuple{var\"#s2\", var\"#s1\"} where {var\"#s2\"<:Function, var\"#s1\"<:Function}}}","page":"Home","title":"AutoComputationalGraphTuning.create_masked_loss_function","text":"create_masked_loss_function(loss_config)\n\nCreate a masked loss function from a configuration named tuple.\n\nArguments\n\nloss_config: Named tuple with loss and agg fields: (loss=loss_function, agg=aggregation_function)\n\nReturns\n\nFunction that computes masked loss: f(predictions, targets, mask)\n\nExamples\n\n# Create MSE loss with mean aggregation\nloss_fn = create_masked_loss_function((loss=Flux.mse, agg=StatsBase.mean))\n\n# Create MAE loss with sum aggregation\nloss_fn = create_masked_loss_function((loss=Flux.mae, agg=sum))\n\n# Use in training\nloss = loss_fn(predictions, targets, mask)\n\n\n\n\n\n","category":"method"},{"location":"#AutoComputationalGraphTuning.evaluate_validation_loss-Tuple{Any, Any}","page":"Home","title":"AutoComputationalGraphTuning.evaluate_validation_loss","text":"evaluate_validation_loss(model, hp, dataloader)\n\nEvaluate validation loss using masked MSE for NaN handling.\n\nArguments\n\nmodel: Trained model instance\nhp: HyperParameters \ndataloader: Validation data loader\n\nReturns\n\nAverage validation loss over all batches\n\n\n\n\n\n","category":"method"},{"location":"#AutoComputationalGraphTuning.evaluate_validation_metrics-Tuple{Any, Any, Any}","page":"Home","title":"AutoComputationalGraphTuning.evaluate_validation_metrics","text":"evaluate_validation_metrics(model, dataloader, n_outputs)\n\nEvaluate comprehensive validation metrics including loss and R² scores.\n\nArguments\n\nmodel: Trained model instance\ndataloader: Validation data loader  \nn_outputs: Number of output targets (RBPs)\n\nReturns\n\navg_loss: Average validation loss\nindividual_r2: R² score for each RBP\naggregated_r2: Overall R² across all predictions\n\n\n\n\n\n","category":"method"},{"location":"#AutoComputationalGraphTuning.finetune_model-Tuple{Any, Any}","page":"Home","title":"AutoComputationalGraphTuning.finetune_model","text":"Fine-tune a pre-trained model with custom loss function.\n\nArguments\n\nmodel: Pre-trained model to fine-tune\nraw_data: Training data\ncompute_loss: Custom loss function with signature (model, seq, labels, mask) -> (loss, aux_info)\nseed: Random seed for reproducibility\nbatch_size: Batch size (if not provided, uses DEFAULTBATCHSIZE)\nmax_epochs: Maximum training epochs\npatience: Early stopping patience\nprint_every: Print frequency\nlearning_rate: Learning rate for fine-tuning (typically smaller than initial training, default: 1e-4)\nnormalize_Y: Whether to normalize Y\nnormalization_method: Normalization method (:zscore or :minmax)\nnormalization_mode: Normalization mode (:rowwise or :columnwise)\nuse_cuda: Use GPU if available\ncombine_train_val: Whether to combine train and validation sets for fine-tuning\n\nReturns\n\nmodel: Fine-tuned model (weights updated in place)\nstats: Training statistics\ntrain_stats: Normalization statistics\n\nExample\n\nusing AutoComputationalGraphTuning\n\n# Load pre-trained model\nmodel = ... # your pre-trained model\n\n# Define custom loss\ncustom_loss = (m, x, y, mask) -> begin\n    output = m(x)\n    preds = output isa Tuple ? output[1] : output\n    loss = masked_mse(preds, y, mask)\n    (loss, Dict(:valid_count => sum(mask)))\nend\n\n# Fine-tune\nfinetuned_model, stats, train_stats = finetune_model(\n    model, data;\n    compute_loss = custom_loss,\n    seed = 42,\n    max_epochs = 20,\n    learning_rate = 1e-4\n)\n\nUsing the gradient regularization loss\n\n# Import custom loss from training module\nusing AutoComputationalGraphTuning: finetune_grad_loss\n\n# Fine-tune with gradient penalty\nfinetuned_model, stats = finetune_model(\n    model, data;\n    compute_loss = (m, x, y, mask) -> finetune_grad_loss(\n        m, x, y, mask; \n        predict_position=1, \n        grad_penalty_weight=0.1\n    ),\n    seed = 42,\n    learning_rate = 5e-5  # Lower learning rate for gradient penalty\n)\n\n\n\n\n\n","category":"method"},{"location":"#AutoComputationalGraphTuning.finetune_model_from_config-Tuple{Any, Any, TrainingConfig}","page":"Home","title":"AutoComputationalGraphTuning.finetune_model_from_config","text":"Fine-tune a pre-trained model from a saved TrainingConfig.\n\nArguments\n\nmodel: Pre-trained model to fine-tune\nraw_data: Training data\nconfig: TrainingConfig from previous training\ncompute_loss: Custom loss function\nmax_epochs: Maximum epochs for fine-tuning\npatience: Early stopping patience\nprint_every: Print frequency\nlearning_rate: Learning rate (default: 1e-4, typically lower than initial training)\n\nReturns\n\nmodel: Fine-tuned model\nstats: Training statistics\n\nExample\n\n# Load config from previous training\nconfig = load_training_config(\"path/to/config.json\")\n\n# Load pre-trained model\nmodel = ... # your model\n\n# Fine-tune with config settings but custom loss\nfinetuned_model, stats = finetune_model_from_config(\n    model, data, config;\n    compute_loss = my_custom_loss,\n    max_epochs = 10,\n    learning_rate = 5e-5\n)\n\n\n\n\n\n","category":"method"},{"location":"#AutoComputationalGraphTuning.get_function_name-Tuple{Any}","page":"Home","title":"AutoComputationalGraphTuning.get_function_name","text":"get_function_name(f)\n\nGet the fully qualified name of a function, preserving module prefix. For common StatsBase re-exports (like mean), prefer StatsBase module name.\n\n\n\n\n\n","category":"method"},{"location":"#AutoComputationalGraphTuning.get_split_indices-Tuple{Int64}","page":"Home","title":"AutoComputationalGraphTuning.get_split_indices","text":"get_split_indices(data_size::Int; train_ratio=0.8, val_ratio=0.1, test_ratio=0.1, shuffle=true, seed=nothing)\n\nReturn shuffled (or ordered) indices for train/validation/test splits.\n\nArguments\n\ndata_size: Number of data points\ntrain_ratio, val_ratio, test_ratio: Proportions for each split (must sum to 1)\nshuffle: Shuffle indices (default: true)\nseed: Random seed (optional)\n\nReturns\n\nNamed tuple: (train, val, test) index vectors\n\nThrows\n\nArgumentError if ratios are invalid or data_size <= 0\n\nExample\n\ntrainidx, validx, testidx = getsplit_indices(1000; seed=42)\n\n\n\n\n\n","category":"method"},{"location":"#AutoComputationalGraphTuning.leading_colons-Tuple{AbstractArray}","page":"Home","title":"AutoComputationalGraphTuning.leading_colons","text":"leading_colons(x::AbstractArray)\n\nReturn a tuple of : (colons) of length ndims(x) - 1. Useful for slicing all but the last dimension of an array.\n\nExamples\n\nA = rand(3, 4, 5)\nA[leading_colons(A)..., 2]  # selects all elements in the last dimension at index 2\n\n\n\n\n\n","category":"method"},{"location":"#AutoComputationalGraphTuning.load_best_trial_config-Tuple{String}","page":"Home","title":"AutoComputationalGraphTuning.load_best_trial_config","text":"load_best_trial_config(save_folder::String)\n\nLoad the configuration for the best trial (highest R²) from a tuning run. Reads the CSV results file and loads the corresponding JSON config.\n\n\n\n\n\n","category":"method"},{"location":"#AutoComputationalGraphTuning.load_trial_config-Tuple{String}","page":"Home","title":"AutoComputationalGraphTuning.load_trial_config","text":"load_trial_config(json_path::String)\n\nLoad a trial configuration from a JSON file.\n\n\n\n\n\n","category":"method"},{"location":"#AutoComputationalGraphTuning.masked_loss","page":"Home","title":"AutoComputationalGraphTuning.masked_loss","text":"masked_loss(predictions, targets, mask, loss_fcn, agg)\n\nApply any Flux loss function only on valid (masked) entries.\n\nArguments\n\npredictions: Model predictions\ntargets: Ground truth targets  \nmask: Boolean mask indicating valid entries\nloss_fcn: Flux loss function (e.g., Flux.mse, Flux.mae, Flux.huber_loss)\nagg: Aggregation function (default: StatsBase.mean)\n\nReturns\n\nLoss computed only on valid entries specified by mask\n\nExamples\n\n# MSE with mean aggregation (default)\nloss = masked_loss(ŷ, y, mask, Flux.mse, StatsBase.mean)\n\n# MAE with sum aggregation  \nloss = masked_loss(ŷ, y, mask, Flux.mae, sum)\n\n# Huber loss with mean aggregation\nloss = masked_loss(ŷ, y, mask, Flux.huber_loss, StatsBase.mean)\n\n\n\n\n\n","category":"function"},{"location":"#AutoComputationalGraphTuning.print_epoch_summary-NTuple{6, Any}","page":"Home","title":"AutoComputationalGraphTuning.print_epoch_summary","text":"Print epoch summary with training and validation metrics\n\n\n\n\n\n","category":"method"},{"location":"#AutoComputationalGraphTuning.save_trial_config-Tuple{TrainingConfig, String}","page":"Home","title":"AutoComputationalGraphTuning.save_trial_config","text":"save_trial_config(config::TrainingConfig, save_folder::String)\n\nSave a trial configuration to a JSON file in the json subfolder.\n\n\n\n\n\n","category":"method"},{"location":"#AutoComputationalGraphTuning.set_reproducible_seeds!","page":"Home","title":"AutoComputationalGraphTuning.set_reproducible_seeds!","text":"Set all random seeds for reproducible results\n\n\n\n\n\n","category":"function"},{"location":"#AutoComputationalGraphTuning.train_batch!-NTuple{4, Any}","page":"Home","title":"AutoComputationalGraphTuning.train_batch!","text":"Train single batch with flexible loss computation.\n\nArguments\n\nmodel: The model to train\nopt_state: Optimizer state\nseq, labels: Batch data\ncompute_loss: Function(model, seq, labels, nanmask) -> (loss, auxinfo)\nShould return loss scalar and optionally auxiliary info dict\nDefault: standard masked MSE loss\n\nReturns\n\nloss: Scalar loss value\naux_info: Dict with auxiliary information (e.g., valid_count, regularizers, etc.)\n\n\n\n\n\n","category":"method"},{"location":"#AutoComputationalGraphTuning.train_epoch!-NTuple{5, Any}","page":"Home","title":"AutoComputationalGraphTuning.train_epoch!","text":"Train single epoch\n\n\n\n\n\n","category":"method"},{"location":"#AutoComputationalGraphTuning.train_final_model-Tuple{Any, Function}","page":"Home","title":"AutoComputationalGraphTuning.train_final_model","text":"Train final model using combined train+val data, evaluate on test set.\n\nReturns: (model, stats, train_stats)\n\n\n\n\n\n","category":"method"},{"location":"#AutoComputationalGraphTuning.train_final_model_from_config-Tuple{Any, Function, TrainingConfig}","page":"Home","title":"AutoComputationalGraphTuning.train_final_model_from_config","text":"Train final model from saved config (e.g., best trial from tuning)\n\n\n\n\n\n","category":"method"},{"location":"#AutoComputationalGraphTuning.train_model-NTuple{5, Any}","page":"Home","title":"AutoComputationalGraphTuning.train_model","text":"Train a model with early stopping and return the best model state and training stats.\n\nArguments\n\ncompute_loss: Optional custom loss function(model, seq, labels, nanmask) -> (loss, auxinfo)\nIf not provided, uses standard masked MSE\nCan return auxiliary info dict for logging custom metrics\nExamples: gradient penalties, attention regularization, multi-task losses\n\nReturns\n\nbest_model_state: State dict of the best model (lowest validation loss)\ntraining_stats: Dict with training history and final metrics\n\nExample Custom Loss\n\n# Gradient penalty example\nfunction my_loss(model, seq, labels, mask)\n    # Forward pass (model can return multiple outputs)\n    output = model(seq)\n    preds = output isa Tuple ? output[1] : output\n    \n    # Standard prediction loss\n    pred_loss = masked_mse(preds, labels, mask)\n    \n    # Custom regularizer (e.g., gradient penalty)\n    grad_penalty = compute_gradient_penalty(model, seq)\n    \n    total_loss = pred_loss + 0.1 * grad_penalty\n    aux = Dict(:pred_loss => pred_loss, :grad_penalty => grad_penalty)\n    \n    (total_loss, aux)\nend\n\n# Use it\ntrain_model(model, opt, train_dl, val_dl, ydim; compute_loss=my_loss)\n\n\n\n\n\n","category":"method"},{"location":"#AutoComputationalGraphTuning.train_val_test_split-Tuple{Any}","page":"Home","title":"AutoComputationalGraphTuning.train_val_test_split","text":"train_val_test_split(data, labels; train_ratio=0.8, val_ratio=0.1, test_ratio=0.1, shuffle=true, seed=nothing)\n\nSplit data and labels together into train/validation/test sets. Uses @views for memory efficiency - returns lightweight views instead of copying data.\n\nArguments\n\ndata: NamedTuple with fields X (features) and Y (labels)\ntrain_ratio, val_ratio, test_ratio: Proportions for each split (must sum to 1)\n_shuffle: Shuffle indices (default: true)\nrng: Random number generator (default: Random.GLOBAL_RNG)\n\nReturns\n\nNamed tuple with (train=(X=..., Y=...), val=(...), test=(...))\nAll data and labels are views (SubArrays) for memory efficiency\n\nExamples\n\ndata = (X = rand(10, 100), Y = rand(1, 100))\nsplits = train_val_test_split(data; seed=42)\ntrain_X = splits.train.X\ntrain_Y = splits.train.Y\n\n\n\n\n\n","category":"method"},{"location":"#AutoComputationalGraphTuning.tune_hyperparameters-Tuple{Any, Function}","page":"Home","title":"AutoComputationalGraphTuning.tune_hyperparameters","text":"Tune hyperparameters across multiple trials with different seeds.\n\nReturns: (resultsdf, bestmodel, best_info)\n\nresults_df: DataFrame with all trial results\nbest_model: Model with highest validation R²\nbestinfo: NamedTuple (seed, r2, batchsize) of best trial\n\n\n\n\n\n","category":"method"}]
}
