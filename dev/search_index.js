var documenterSearchIndex = {"docs":
[{"location":"#AutoComputationalGraphTuning","page":"Home","title":"AutoComputationalGraphTuning","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Documentation for AutoComputationalGraphTuning.","category":"page"},{"location":"#Quick-Start","page":"Home","title":"Quick Start","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"This package makes hyperparameter tuning for Flux models simple. You just need a data struct and a model creation function.","category":"page"},{"location":"#What-You-Need","page":"Home","title":"What You Need","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Data struct with four fields:","category":"page"},{"location":"","page":"Home","title":"Home","text":"data.X, data.Y - your features and labels\ndata.X_dim, data.Y_dim - dimensions of each feature/label","category":"page"},{"location":"","page":"Home","title":"Home","text":"Model function that creates a Flux model:","category":"page"},{"location":"","page":"Home","title":"Home","text":"function create_model(X_dim, Y_dim, batch_size; rng, use_cuda)\n    # Your model code here\n    return model  # Must have a `linear_sum` property\nend","category":"page"},{"location":"#Loss-Function-Flexibility","page":"Home","title":"Loss Function Flexibility","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Want to experiment with different loss functions? Easy:","category":"page"},{"location":"","page":"Home","title":"Home","text":"# Default MSE\ntune_hyperparameters(data, create_model)\n\n# Try MAE instead (less sensitive to outliers)\ntune_hyperparameters(data, create_model; \n                    loss_fcn=(loss=Flux.mae, agg=StatsBase.mean))\n\n# Huber loss (robust choice)\ntune_hyperparameters(data, create_model;\n                    loss_fcn=(loss=Flux.huber_loss, agg=StatsBase.mean))","category":"page"},{"location":"","page":"Home","title":"Home","text":"The loss_fcn parameter takes any Flux loss function with your choice of aggregation. Works the same way in train_final_model() too.","category":"page"},{"location":"#Basic-Workflow","page":"Home","title":"Basic Workflow","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"# 1. Tune hyperparameters\nresults = tune_hyperparameters(data, create_model; \n                              max_epochs=50, n_trials=100)\n\n# 2. Train final model  \nmodel, stats = train_final_model(data, create_model; \n                                seed=42, max_epochs=100)","category":"page"},{"location":"","page":"Home","title":"Home","text":"The package handles data splitting, normalization, early stopping, and model selection automatically.","category":"page"},{"location":"","page":"Home","title":"Home","text":"","category":"page"},{"location":"#AutoComputationalGraphTuning.DataSplit","page":"Home","title":"AutoComputationalGraphTuning.DataSplit","text":"DataSplit\n\nRepresents a single data split with tensor and labels.\n\nFields\n\ntensor: Encoded sequence tensor (e.g. 4D array)\nlabels: Labels (vector or matrix)\nstats: Normalization statistics (only present for training data when normalized)\n\n\n\n\n\n","category":"type"},{"location":"#AutoComputationalGraphTuning.PreprocessedData","page":"Home","title":"AutoComputationalGraphTuning.PreprocessedData","text":"PreprocessedData\n\nContainer for preprocessed train/validation/test splits.\n\nFields\n\ntrain: Training data split\nval: Validation data split\ntest: Test data split\n\n\n\n\n\n","category":"type"},{"location":"#AutoComputationalGraphTuning.check_early_stopping!-NTuple{9, Any}","page":"Home","title":"AutoComputationalGraphTuning.check_early_stopping!","text":"Check early stopping condition and update best model state\n\n\n\n\n\n","category":"method"},{"location":"#AutoComputationalGraphTuning.compute_r2_scores-Tuple{Any, Any, Any}","page":"Home","title":"AutoComputationalGraphTuning.compute_r2_scores","text":"compute_r2_scores(predictions, targets, mask)\n\nCompute R² scores for individual RBPs and aggregated across all predictions.\n\nArguments\n\npredictions: Model predictions matrix (nrbps, nsamples)\ntargets: Ground truth targets matrix (nrbps, nsamples)  \nmask: Boolean mask for valid entries (nrbps, nsamples)\n\nReturns\n\nindividual_r2: R² score for each RBP (may contain NaN for insufficient data)\naggregated_r2: Overall R² across all valid predictions\n\n\n\n\n\n","category":"method"},{"location":"#AutoComputationalGraphTuning.create_masked_loss_function-Tuple{NamedTuple{(:loss, :agg), <:Tuple{var\"#s4\", var\"#s3\"} where {var\"#s4\"<:Function, var\"#s3\"<:Function}}}","page":"Home","title":"AutoComputationalGraphTuning.create_masked_loss_function","text":"create_masked_loss_function(loss_config)\n\nCreate a masked loss function from a configuration named tuple.\n\nArguments\n\nloss_config: Named tuple with loss and agg fields: (loss=loss_function, agg=aggregation_function)\n\nReturns\n\nFunction that computes masked loss: f(predictions, targets, mask)\n\nExamples\n\n# Create MSE loss with mean aggregation\nloss_fn = create_masked_loss_function((loss=Flux.mse, agg=StatsBase.mean))\n\n# Create MAE loss with sum aggregation\nloss_fn = create_masked_loss_function((loss=Flux.mae, agg=sum))\n\n# Use in training\nloss = loss_fn(predictions, targets, mask)\n\n\n\n\n\n","category":"method"},{"location":"#AutoComputationalGraphTuning.evaluate_validation_loss-Tuple{Any, Any}","page":"Home","title":"AutoComputationalGraphTuning.evaluate_validation_loss","text":"evaluate_validation_loss(model, hp, dataloader)\n\nEvaluate validation loss using masked MSE for NaN handling.\n\nArguments\n\nmodel: Trained model instance\nhp: HyperParameters \ndataloader: Validation data loader\n\nReturns\n\nAverage validation loss over all batches\n\n\n\n\n\n","category":"method"},{"location":"#AutoComputationalGraphTuning.evaluate_validation_metrics-Tuple{Any, Any, Any}","page":"Home","title":"AutoComputationalGraphTuning.evaluate_validation_metrics","text":"evaluate_validation_metrics(model, dataloader, n_outputs)\n\nEvaluate comprehensive validation metrics including loss and R² scores.\n\nArguments\n\nmodel: Trained model instance\ndataloader: Validation data loader  \nn_outputs: Number of output targets (RBPs)\n\nReturns\n\navg_loss: Average validation loss\nindividual_r2: R² score for each RBP\naggregated_r2: Overall R² across all predictions\n\n\n\n\n\n","category":"method"},{"location":"#AutoComputationalGraphTuning.get_split_indices-Tuple{Int64}","page":"Home","title":"AutoComputationalGraphTuning.get_split_indices","text":"get_split_indices(data_size::Int; train_ratio=0.8, val_ratio=0.1, test_ratio=0.1, shuffle=true, seed=nothing)\n\nReturn shuffled (or ordered) indices for train/validation/test splits.\n\nArguments\n\ndata_size: Number of data points\ntrain_ratio, val_ratio, test_ratio: Proportions for each split (must sum to 1)\nshuffle: Shuffle indices (default: true)\nseed: Random seed (optional)\n\nReturns\n\nNamed tuple: (train, val, test) index vectors\n\nThrows\n\nArgumentError if ratios are invalid or data_size <= 0\n\nExample\n\ntrainidx, validx, testidx = getsplit_indices(1000; seed=42)\n\n\n\n\n\n","category":"method"},{"location":"#AutoComputationalGraphTuning.leading_colons-Tuple{AbstractArray}","page":"Home","title":"AutoComputationalGraphTuning.leading_colons","text":"leading_colons(x::AbstractArray)\n\nReturn a tuple of : (colons) of length ndims(x) - 1. Useful for slicing all but the last dimension of an array.\n\nExamples\n\nA = rand(3, 4, 5)\nA[leading_colons(A)..., 2]  # selects all elements in the last dimension at index 2\n\n\n\n\n\n","category":"method"},{"location":"#AutoComputationalGraphTuning.masked_loss","page":"Home","title":"AutoComputationalGraphTuning.masked_loss","text":"masked_loss(predictions, targets, mask, loss_fcn, agg)\n\nApply any Flux loss function only on valid (masked) entries.\n\nArguments\n\npredictions: Model predictions\ntargets: Ground truth targets  \nmask: Boolean mask indicating valid entries\nloss_fcn: Flux loss function (e.g., Flux.mse, Flux.mae, Flux.huber_loss)\nagg: Aggregation function (default: StatsBase.mean)\n\nReturns\n\nLoss computed only on valid entries specified by mask\n\nExamples\n\n# MSE with mean aggregation (default)\nloss = masked_loss(ŷ, y, mask, Flux.mse, StatsBase.mean)\n\n# MAE with sum aggregation  \nloss = masked_loss(ŷ, y, mask, Flux.mae, sum)\n\n# Huber loss with mean aggregation\nloss = masked_loss(ŷ, y, mask, Flux.huber_loss, StatsBase.mean)\n\n\n\n\n\n","category":"function"},{"location":"#AutoComputationalGraphTuning.print_epoch_summary-NTuple{6, Any}","page":"Home","title":"AutoComputationalGraphTuning.print_epoch_summary","text":"Print epoch summary with training and validation metrics\n\n\n\n\n\n","category":"method"},{"location":"#AutoComputationalGraphTuning.set_reproducible_seeds!","page":"Home","title":"AutoComputationalGraphTuning.set_reproducible_seeds!","text":"Set all random seeds for reproducible results\n\n\n\n\n\n","category":"function"},{"location":"#AutoComputationalGraphTuning.train_batch!-NTuple{4, Any}","page":"Home","title":"AutoComputationalGraphTuning.train_batch!","text":"Train single batch and return loss and statistics\n\n\n\n\n\n","category":"method"},{"location":"#AutoComputationalGraphTuning.train_epoch!-NTuple{5, Any}","page":"Home","title":"AutoComputationalGraphTuning.train_epoch!","text":"Train single epoch and return epoch statistics\n\n\n\n\n\n","category":"method"},{"location":"#AutoComputationalGraphTuning.train_model-NTuple{5, Any}","page":"Home","title":"AutoComputationalGraphTuning.train_model","text":"train_model(model, opt_state, train_dl, val_dl, output_dim; \n            max_epochs=50, patience=10, min_delta=1e-4, print_every=100)\n\nTrain a model with early stopping and return the best model state and training stats.\n\nReturns\n\nbest_model_state: State dict of the best model (lowest validation loss)\ntraining_stats: Dict with training history and final metrics\n\n\n\n\n\n","category":"method"},{"location":"#AutoComputationalGraphTuning.train_val_test_split-Tuple{Any}","page":"Home","title":"AutoComputationalGraphTuning.train_val_test_split","text":"train_val_test_split(data, labels; train_ratio=0.8, val_ratio=0.1, test_ratio=0.1, shuffle=true, seed=nothing)\n\nSplit data and labels together into train/validation/test sets. Uses @views for memory efficiency - returns lightweight views instead of copying data.\n\nArguments\n\ndata: NamedTuple with fields X (features) and Y (labels)\ntrain_ratio, val_ratio, test_ratio: Proportions for each split (must sum to 1)\n_shuffle: Shuffle indices (default: true)\nrng: Random number generator (default: Random.GLOBAL_RNG)\n\nReturns\n\nNamed tuple with (train=(X=..., Y=...), val=(...), test=(...))\nAll data and labels are views (SubArrays) for memory efficiency\n\nExamples\n\ndata = (X = rand(10, 100), Y = rand(1, 100))\nsplits = train_val_test_split(data; seed=42)\ntrain_X = splits.train.X\ntrain_Y = splits.train.Y\n\n\n\n\n\n","category":"method"},{"location":"#AutoComputationalGraphTuning.tune_hyperparameters-Tuple{Any, Function}","page":"Home","title":"AutoComputationalGraphTuning.tune_hyperparameters","text":"tune_hyperparameters(raw_data::SEQ2EXP_Dataset; seq_type=:Nucleotide, randomize_batchsize=false, max_epochs=50, patience=5, trial_number_start=1, n_trials=10, suppress_warnings=false, save_folder=nothing)\n\nTune hyperparameters for a model using train/validation split. Runs multiple trials, each with a different random seed, and tracks the best validation R² score and loss. Optionally saves results to CSV.\n\nArguments\n\nraw_data::SEQ2EXP_Dataset: The dataset to use for training/validation\nseq_type: Sequence type (default: :Nucleotide)\nrandomize_batchsize: Whether to randomize batch size per trial\nmax_epochs: Maximum epochs per trial\npatience: Early stopping patience\ntrial_number_start: Starting trial number/seed\nn_trials: Number of trials to run\nsuppress_warnings: Suppress warnings during setup\nsave_folder: If provided, results are saved to this folder as CSV\n\nReturns\n\nresults_df: DataFrame with trial, best R², and validation loss for each trial\n\n\n\n\n\n","category":"method"}]
}
