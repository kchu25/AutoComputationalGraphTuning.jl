# AutoComputationalGraphTuning

[![Stable](https://img.shields.io/badge/docs-stable-blue.svg)](https://kchu25.github.io/AutoComputationalGraphTuning.jl/stable/)
[![Dev](https://img.shields.io/badge/docs-dev-blue.svg)](https://kchu25.github.io/AutoComputationalGraphTuning.jl/dev/)
[![Build Status](https://github.com/kchu25/AutoComputationalGraphTuning.jl/actions/workflows/CI.yml/badge.svg?branch=main)](https://github.com/kchu25/AutoComputationalGraphTuning.jl/actions/workflows/CI.yml?query=branch%3Amain)
[![Coverage](https://codecov.io/gh/kchu25/AutoComputationalGraphTuning.jl/branch/main/graph/badge.svg)](https://codecov.io/gh/kchu25/AutoComputationalGraphTuning.jl)

This package just requires two things: a `data` struct and a function to create a Flux model. 
Following duck typing practices, these must satisfy the following:

## Requirements

### Data Structure
Your `data` struct needs four fields:
- `data.X` - the features
- `data.Y` - the labels  
- `data.X_dim` - dimension of each feature
- `data.Y_dim` - dimension of each label

### Model Creation Function
You need a `create_model` function that:
- Takes `(X_dim, Y_dim, batch_size; rng, use_cuda)` as arguments
- Returns a Flux model
- Must define a `linear_sum` property

## Loss Function Configuration

Want to use a different loss function? No problem! You can configure any Flux loss function with custom aggregation:

```julia
# Default: MSE with mean aggregation
tune_hyperparameters(data, create_model)

# Use MAE instead
tune_hyperparameters(data, create_model; 
                    loss_fcn=(loss=Flux.mae, agg=StatsBase.mean))

# Huber loss (robust to outliers)
tune_hyperparameters(data, create_model;
                    loss_fcn=(loss=Flux.huber_loss, agg=StatsBase.mean))

# MSE with sum aggregation instead of mean
tune_hyperparameters(data, create_model;
                    loss_fcn=(loss=Flux.mse, agg=sum))
```

The `loss_fcn` parameter is a named tuple with:
- `loss`: Any Flux loss function (`Flux.mse`, `Flux.mae`, `Flux.huber_loss`, etc.)
- `agg`: Aggregation function (`StatsBase.mean`, `sum`, `identity`, or your own function)

This works for both `tune_hyperparameters()` and `train_final_model()` - just pass the same `loss_fcn` parameter!

## Basic Usage

```julia
# Tune hyperparameters with automatic config saving
results = tune_hyperparameters(data, create_model; 
                              max_epochs=50, 
                              n_trials=100,
                              save_folder="results/my_tuning_run")

# Train final model with best hyperparameters
model, stats = train_final_model(data, create_model; 
                                seed=42, 
                                max_epochs=100)
```

## Advanced: Load Best Trial Configuration

When you run tuning with `save_folder`, the package automatically saves:
- CSV file with all trial results
- JSON file for each trial in a `json/` subfolder with complete configuration

You can load the best trial's configuration and use it for final training:

```julia
# Run tuning and save configs
tune_hyperparameters(data, create_model; 
                    n_trials=100,
                    save_folder="results/experiment_001")

# Later, load the best trial configuration
config = load_best_trial_config("results/experiment_001")

# Train final model using the exact same settings as the best trial
model, stats = train_final_model_from_config(data, create_model, config;
                                            max_epochs=200,
                                            patience=20)
```

Each trial's JSON config includes:
- Seed (for reproducibility)
- Normalization settings (`normalize_Y`, `normalization_method`, `normalization_mode`)
- Hardware settings (`use_cuda`)
- Batch size settings (`randomize_batchsize`, actual `batch_size` used)
- Loss function configuration (`loss_function`, `aggregation`)
- Trial results (`best_r2`, `val_loss`)

That's it! The package handles data splitting, normalization, early stopping, and gives you flexibility over loss functions while keeping the API simple.

